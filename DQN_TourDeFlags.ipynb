{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning - The Tour De Flags Game\n",
    "\n",
    "### References\n",
    "- https://samyzaf.com/ML/tdf/tdf.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Game\n",
    "\n",
    "The Tour De Flags maze is a game in which an **Agent** collect several flags before arriving to the target cell. For simplicity, we will assume the agent always starts at cell (0,0) and its destination cell is always at the bottom right cell of the maze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os, sys, time, datetime, json, random\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD , Adam, RMSprop\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.activations import relu\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environement Description\n",
    "\n",
    "A framework for an MDP (Markov Decision Process) consists of an **environment** and an **agent** which acts in this environment. In our case the environment is a classical square maze with three types of cells:\n",
    "\n",
    "1. Occupied cells\n",
    "1. Free cells\n",
    "2. Flag cells\n",
    "3. Target cells\n",
    "\n",
    "Our agent is is allowed to move only on free cells and on flag cells (for picking the flags). After picking all flags, our agent must head to the target cell. **The fastest he completes this mission the highet is his rewards.**\n",
    "\n",
    "In our model, the agent is \"encouraged\" to find the shortest path to the target cell by a simple **rewarding scheme**:\n",
    "\n",
    "1. We have exactly 4 actions which we must encode as integers 0-3:\n",
    "    - 0 - left\n",
    "    - 1 - up\n",
    "    - 2 - right\n",
    "    - 3 - down\n",
    "2. Our rewards will be floating points ranging from -1.0 to 1.0.\n",
    "3. Each move from one state to the next state will be rewarded by a small negative reward, so that the agent is encouraged to minimized the number of moves.\n",
    "3. The agent will be awarded a positive reward immediately after capturing each flag.\n",
    "3. The maximal reward of 1.0 points is given when the rat hits the cheese cell. However, a negative reward will be added for every flag which was not captured.\n",
    "3. An attempt to enter a blocked cell (\"red\" cell) will cost a significant negative reward.\n",
    "3. Same rule hold for an attempt to move outside the maze boundaries.\n",
    "3. To avoid infinite loops and senseless wandering, the game is ended (lose) once the total reward of the rat is below the negative threshold.\n",
    "3. The awards policy is precisely defined in the **reward** and **get_reward** methods of the Tmaze class\n",
    "\n",
    "Here are our main constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gray scale marks for cells\n",
    "visited_mark = 0.9\n",
    "flag_mark = 0.65\n",
    "agent_mark = 0.5\n",
    "\n",
    "# Actions dictionary\n",
    "actions_dict = {\n",
    "    0: 'left',\n",
    "    1: 'up',\n",
    "    2: 'right',\n",
    "    3: 'down',\n",
    "}\n",
    "\n",
    "num_actions = len(actions_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tmaze Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maze is a 2d Numpy array of floats between 0.0 to 1.0\n",
    "# 1.0 corresponds to a free cell, and 0.0 an occupied cell\n",
    "# agent = (row, col) initial agent position (defaults to (0,0))\n",
    "\n",
    "class Tmaze(object):\n",
    "    \"\"\"\n",
    "    Tour De Flags maze object\n",
    "    maze: a 2d Numpy array of 0's and 1's\n",
    "        1.00 - a free cell\n",
    "        0.65 - flag cell\n",
    "        0.50 - agent cell\n",
    "        0.00 - an occupied cell\n",
    "    agent: (row, col) initial agent position (defaults to (0,0))\n",
    "    flags: list of cells occupied by flags\n",
    "    \"\"\"\n",
    "    def __init__(self, maze, flags, agent=(0,0), target=None):\n",
    "        self._maze = np.array(maze)\n",
    "        self._flags = set(flags)\n",
    "        nrows, ncols = self._maze.shape\n",
    "        if target is None:\n",
    "            self.target = (nrows-1, ncols-1)   # default target cell where the agent to deliver the \"flags\"\n",
    "        self.free_cells = set((r,c) for r in range(nrows) for c in range(ncols) if self._maze[r,c] == 1.0)\n",
    "        self.free_cells.discard(self.target)\n",
    "        self.free_cells -= self._flags\n",
    "        if self._maze[self.target] == 0.0:\n",
    "            raise Exception(\"Invalid maze: target cell cannot be blocked!\")\n",
    "        if not agent in self.free_cells:\n",
    "            raise Exception(\"Invalid agent Location: must sit on a free cell\")\n",
    "        self.reset(agent)\n",
    "\n",
    "    def reset(self, agent=(0,0)):\n",
    "        self.agent = agent\n",
    "        self.maze = np.copy(self._maze)\n",
    "        self.flags = set(self._flags)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        row, col = agent\n",
    "        self.maze[row, col] = agent_mark\n",
    "        self.state = ((row, col), 'start')\n",
    "        self.base = np.sqrt(self.maze.size)\n",
    "        self.visited = dict(((r,c),0) for r in range(nrows) for c in range(ncols) if self._maze[r,c] == 1.0)\n",
    "        self.total_reward = 0\n",
    "        self.min_reward = -0.5 * self.maze.size\n",
    "        print('[TMaze.self.min_reward]', self.min_reward)\n",
    "        self.reward = {\n",
    "            'blocked':  self.min_reward,\n",
    "            'flag':     1.0/len(self._flags),\n",
    "            'invalid': -4.0/self.base,\n",
    "            'valid':   -1.0/self.maze.size\n",
    "        }\n",
    "        print('[TMaze.self.reward]', self.reward)\n",
    "\n",
    "    def act(self, action):\n",
    "        self.update_state(action)\n",
    "        reward = self.get_reward()\n",
    "        self.total_reward += reward\n",
    "        status = self.game_status()\n",
    "        env_state = self.observe()\n",
    "        return env_state, reward, status\n",
    "\n",
    "    def get_reward(self):\n",
    "        agent, mode = self.state\n",
    "        if agent == self.target:\n",
    "            return 1.0 - len(self.flags) / len(self._flags)\n",
    "        if mode == 'blocked':\n",
    "            return self.reward['blocked']\n",
    "        elif agent in self.flags:\n",
    "            return self.reward['flag']\n",
    "        elif mode == 'invalid':\n",
    "            return self.reward['invalid']\n",
    "        elif mode == 'valid':\n",
    "            return self.reward['valid'] #* (1 + 0.1*self.visited[agent] ** 2)\n",
    "\n",
    "    def update_state(self, action):\n",
    "        nrows, ncols = self.maze.shape\n",
    "        (nrow, ncol), nmode = agent, mode = self.state\n",
    "\n",
    "        if self.maze[agent] > 0.0:\n",
    "            self.visited[agent] += 1  # mark visited cell\n",
    "        if agent in self.flags:\n",
    "            self.flags.remove(agent)\n",
    "\n",
    "        valid_actions = self.valid_actions()\n",
    "\n",
    "        if not valid_actions:\n",
    "            nmode = 'blocked'\n",
    "        elif action in valid_actions:\n",
    "            nmode = 'valid'\n",
    "            if action == 0:    # move left\n",
    "                ncol -= 1\n",
    "            elif action == 1:  # move up\n",
    "                nrow -= 1\n",
    "            elif action == 2:    # move right\n",
    "                ncol += 1\n",
    "            elif action == 3:  # move down\n",
    "                nrow += 1\n",
    "        else:                  # invalid action, no change in agent position\n",
    "            nmode = 'invalid'\n",
    "\n",
    "        # new state\n",
    "        agent = (nrow, ncol)\n",
    "        self.state = (agent, nmode)\n",
    "\n",
    "    def game_status(self):\n",
    "        if self.total_reward < self.min_reward:\n",
    "            return 'lose'\n",
    "        agent, mode = self.state\n",
    "        if agent == self.target:\n",
    "            if len(self.flags) == 0:\n",
    "                return 'win'\n",
    "            else:\n",
    "                return 'lose'\n",
    "\n",
    "        return 'ongoing'\n",
    "\n",
    "    def observe(self):\n",
    "        canvas = self.draw_env()\n",
    "        env_state = canvas.reshape((1, -1))\n",
    "        return env_state\n",
    "\n",
    "    def draw_env(self):\n",
    "        canvas = np.copy(self.maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        # clear all visual marks\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                if canvas[r,c] > 0.0:\n",
    "                    canvas[r,c] = 1.0\n",
    "        # draw the flags\n",
    "        for r,c in self.flags:\n",
    "            canvas[r,c] = flag_mark\n",
    "        # draw the agent\n",
    "        agent, mode = self.state\n",
    "        canvas[agent] = agent_mark\n",
    "        return canvas\n",
    "\n",
    "    def valid_actions(self, cell=None):\n",
    "        if cell is None:\n",
    "            (row, col), mode = self.state\n",
    "        else:\n",
    "            row, col = cell\n",
    "        actions = [0, 1, 2, 3]\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if row == 0:\n",
    "            actions.remove(1)\n",
    "        elif row == nrows-1:\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col == 0:\n",
    "            actions.remove(0)\n",
    "        elif col == ncols-1:\n",
    "            actions.remove(2)\n",
    "\n",
    "        if row>0 and self.maze[row-1,col] == 0.0:\n",
    "            actions.remove(1)\n",
    "        if row<nrows-1 and self.maze[row+1,col] == 0.0:\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col>0 and self.maze[row,col-1] == 0.0:\n",
    "            actions.remove(0)\n",
    "        if col<ncols-1 and self.maze[row,col+1] == 0.0:\n",
    "            actions.remove(2)\n",
    "\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experience(object):\n",
    "    def __init__(self, model, max_memory=100, discount=0.97):\n",
    "        self.model = model\n",
    "        self.max_memory = max_memory\n",
    "        self.discount = discount\n",
    "        self.memory = list()\n",
    "        self.num_actions = model.output_shape[-1]\n",
    "\n",
    "    def remember(self, episode):\n",
    "        # episode = [env_state, action, reward, next_env_state, game_over]\n",
    "        # memory[i] = episode\n",
    "        # env_state == flattened 1d maze cells info, including agent cell (see method: observe)\n",
    "        self.memory.append(episode)\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def predict(self, env_state):\n",
    "        return self.model.predict(env_state)[0]\n",
    "\n",
    "    def get_data(self, data_size=10):\n",
    "        env_size = self.memory[0][0].shape[1]   # env_state 1d size (1st element of episode)\n",
    "        mem_size = len(self.memory)\n",
    "        data_size = min(mem_size, data_size)\n",
    "        inputs = np.zeros((data_size, env_size))\n",
    "        targets = np.zeros((data_size, self.num_actions))\n",
    "        for i, j in enumerate(np.random.choice(range(mem_size), data_size, replace=False)):\n",
    "            env_state, action, reward, next_env_state, game_over = self.memory[j]\n",
    "            inputs[i] = env_state\n",
    "            # There should be no target values for actions not taken.\n",
    "            # Thou shalt not correct actions not taken #deep (quote by Eder Santana)\n",
    "            targets[i] = self.predict(env_state)\n",
    "            # Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\n",
    "            Q_sa = np.max(self.predict(next_env_state))\n",
    "            if game_over:\n",
    "                targets[i, action] = reward\n",
    "            else:\n",
    "                # reward + gamma * max_a' Q(s', a')\n",
    "                targets[i, action] = reward + self.discount * Q_sa\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qtraining(object):\n",
    "    def __init__(self, model, env, **opt):\n",
    "        self.model = model  # Nueral Network Model\n",
    "        self.env = env  # Environment (Tour De Flags maze object)\n",
    "        self.n_epoch = opt.get('n_epoch', 1000)  # Number of epochs to run\n",
    "        self.max_memory = opt.get('max_memory', 4*self.env.maze.size)  # Max memory for experiences\n",
    "        self.data_size = opt.get('data_size', int(0.75*self.env.maze.size))  # Data samples from experience replay\n",
    "        self.agent_cells = opt.get('agent_cells', [(0,0)])  # Starting cells for the agent\n",
    "        self.weights_file = opt.get('weights_file', \"\")  # Keras model weights file\n",
    "        self.name = opt.get('name', 'model')  # Name for saving weights and json files\n",
    "\n",
    "        self.win_count = 0\n",
    "        # If you want to continue training from a previous model,\n",
    "        # just supply the h5 file name to weights_file option\n",
    "        if self.weights_file:\n",
    "            print(\"loading weights from file: %s\" % (self.weights_file,))\n",
    "            self.model.load_weights(self.weights_file)\n",
    "\n",
    "        if self.agent_cells == 'all':\n",
    "            self.agent_cells = self.env.free_cells\n",
    "\n",
    "        # Initialize experience replay object\n",
    "        self.experience = Experience(self.model, max_memory=self.max_memory)\n",
    "\n",
    "    def train(self):\n",
    "        start_time = datetime.datetime.now()\n",
    "        self.seconds = 0\n",
    "        self.win_count = 0\n",
    "        for epoch in range(self.n_epoch):\n",
    "            self.epoch = epoch\n",
    "            self.loss = 0.0\n",
    "            agent = random.choice(self.agent_cells)\n",
    "            self.env.reset(agent)\n",
    "            game_over = False\n",
    "            # get initial env_state (1d flattened canvas)\n",
    "            self.env_state = self.env.observe()\n",
    "            self.n_episodes = 0\n",
    "            while not game_over:\n",
    "                game_over = self.play()\n",
    "\n",
    "            dt = datetime.datetime.now() - start_time\n",
    "            self.seconds = dt.total_seconds()\n",
    "            t = format_time(self.seconds)\n",
    "            fmt = \"Epoch: {:3d}/{:d} | Loss: {:.4f} | Episodes: {:4d} | Wins: {:2d} | flags: {:d} | e: {:.3f} | time: {}\"\n",
    "            print(fmt.format(epoch, self.n_epoch-1, self.loss, self.n_episodes, self.win_count, len(self.env.flags), self.epsilon(), t))\n",
    "            if self.win_count > 2:\n",
    "                if self.completion_check():\n",
    "                    print(\"Completed training at epoch: %d\" % (epoch,))\n",
    "                    break\n",
    "\n",
    "    def play(self):\n",
    "        action = self.action()\n",
    "        prev_env_state = self.env_state\n",
    "        self.env_state, reward, game_status = self.env.act(action)\n",
    "        if game_status == 'win':\n",
    "            self.win_count += 1\n",
    "            game_over = True\n",
    "        elif game_status == 'lose':\n",
    "            game_over = True\n",
    "        else:\n",
    "            game_over = False\n",
    "\n",
    "        # Store episode (experience)\n",
    "        episode = [prev_env_state, action, reward, self.env_state, game_over]\n",
    "        self.experience.remember(episode)\n",
    "        self.n_episodes += 1\n",
    "\n",
    "        # Train model\n",
    "        inputs, targets = self.experience.get_data(data_size=self.data_size)\n",
    "        epochs = int(self.env.base)\n",
    "        h = self.model.fit(\n",
    "            inputs,\n",
    "            targets,\n",
    "            epochs = epochs,\n",
    "            batch_size=16,\n",
    "            verbose=0,\n",
    "        )\n",
    "        self.loss = self.model.evaluate(inputs, targets, verbose=0)\n",
    "        return game_over\n",
    "\n",
    "    def run_game(self, agent):\n",
    "        self.env.reset(agent)\n",
    "        env_state = self.env.observe()\n",
    "        while True:\n",
    "            # get next action\n",
    "            q = self.model.predict(env_state)\n",
    "            action = np.argmax(q[0])\n",
    "            prev_env_state = env_state\n",
    "            # apply action, get rewards and new state\n",
    "            env_state, reward, game_status = self.env.act(action)\n",
    "            if game_status == 'win':\n",
    "                return True\n",
    "            elif game_status == 'lose':\n",
    "                return False\n",
    "\n",
    "    def action(self):\n",
    "        # Get next action\n",
    "        valid_actions = self.env.valid_actions()\n",
    "        if not valid_actions:\n",
    "            action = None\n",
    "        elif np.random.rand() < self.epsilon():\n",
    "            action = random.choice(valid_actions)\n",
    "        else:\n",
    "            q = self.experience.predict(self.env_state)\n",
    "            action = np.argmax(q)\n",
    "        return action\n",
    "\n",
    "    def epsilon(self):\n",
    "        n = self.win_count\n",
    "        top = 0.80\n",
    "        bottom = 0.08\n",
    "        if n<10:\n",
    "            e = bottom + (top - bottom) / (1 + 0.1 * n**0.5)\n",
    "        else:\n",
    "            e = bottom\n",
    "        return e\n",
    "    \n",
    "    def completion_check(self):\n",
    "        for agent in self.agent_cells:\n",
    "            if not self.run_game(agent):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def save(self, name=\"\"):\n",
    "        # Save trained model weights and architecture, this will be used by the visualization code\n",
    "        if not name:\n",
    "            name = self.name\n",
    "        h5file = 'model_%s.h5' % (name,)\n",
    "        json_file = 'model_%s.json' % (name,)\n",
    "        self.model.save_weights(h5file, overwrite=True)\n",
    "        with open(json_file, \"w\") as outfile:\n",
    "            json.dump(self.model.to_json(), outfile)\n",
    "        t = format_time(self.seconds)\n",
    "        print('files: %s, %s' % (h5file, json_file))\n",
    "        print(\"n_epoch: %d, max_mem: %d, data: %d, time: %s\" % (self.epoch, self.max_memory, self.data_size, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN model\n",
    "\n",
    "Choosing the correct parameters for a suitable model is not easy and requires some experience and many experiments. In the case of the Tour De Flags maze we found that:\n",
    "1. The most suitable activation function is SReLU (the S-shaped relu), but it has been removed from the latest Keras version, so we have used LeakyRelu instead (the closest thing)\n",
    "2. Our optimizer is adam\n",
    "3. Our loss function is mse (Mean Squared Error).\n",
    "\n",
    "We use two hidden layers, each of size equals to the maze size. \n",
    "\n",
    "The input layer has also the same size as the maze since it accepts the maze state as input. \n",
    "\n",
    "The output layer size is the same as the number of actions (4 in our case), since its outputs the estimated q-value for each action. (we need to take the action with the maximal q-value for playing in the game)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(env, **opt):\n",
    "    loss = opt.get('loss', 'mse')\n",
    "    a = opt.get('alpha', 0.24)\n",
    "    model = Sequential()\n",
    "    esize = env.maze.size\n",
    "    \n",
    "    model.add(Dense(esize, input_shape=(esize,)))\n",
    "    model.add(LeakyReLU(alpha=a))\n",
    "    model.add(Dense(esize))\n",
    "    model.add(LeakyReLU(alpha=a))\n",
    "    model.add(Dense(num_actions))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_env(env, fname=None):\n",
    "    plt.grid('on')\n",
    "    n = env.maze.shape[0]\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(0.5, n, 1))\n",
    "    ax.set_yticks(np.arange(0.5, n, 1))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    canvas = np.copy(env.maze)\n",
    "    for cell in env.visited:\n",
    "        if env.visited[cell]:\n",
    "            canvas[cell] = visited_mark\n",
    "    for cell in env.flags:\n",
    "        canvas[cell] = flag_mark\n",
    "    img = plt.imshow(canvas, interpolation='none', cmap='gray')\n",
    "    if fname:\n",
    "        plt.savefig(fname)\n",
    "    return img\n",
    "\n",
    "def format_time(seconds):\n",
    "    if seconds < 400:\n",
    "        s = float(seconds)\n",
    "        return \"%.1f seconds\" % (s,)\n",
    "    elif seconds < 4000:\n",
    "        m = seconds / 60.0\n",
    "        return \"%.2f minutes\" % (m,)\n",
    "    else:\n",
    "        h = seconds / 3600.0\n",
    "        return \"%.2f hours\" % (h,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of maze: (7, 7)\n",
      "[TMaze.self.min_reward] -24.5\n",
      "[TMaze.self.reward] {'blocked': -24.5, 'flag': 0.25, 'invalid': -0.5714285714285714, 'valid': -0.02040816326530612}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14950cb8e08>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOsAAADrCAYAAACICmHVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy86wFpkAAAACXBIWXMAAAsTAAALEwEAmpwYAAAFMElEQVR4nO3dMU4bWQDG8TcWBCkggRCSFUSVZg9gejhNuID7XGBOkAuEiivgA5gDbJcCEUVCVENB9bbJSlnFmCAtPD7795OmsqVvBvNPTPW6WmsB3r5R6xsA/oxYIYRYIYRYIYRYIYRYIcTGc978/v37ure390K3stzu7m7Z3t5usn1/f2/b9qv49u1bub297Ra99qxY9/b2ytnZ2f9zV890cnJSTk9Pm2zPZjPbtl/F8fHxo6/5GgwhxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohnnUw1eHhYfn8+fNL3ctSs9msyS68FV2tdfkbuu5TKeVTKaWMx+PJ+fn5a9zXb4ZhKDs7O7Ztr/T2dDot8/l84ZGPpdb6x9dkMqmtXF5e2ra98ts/G1vYn79ZIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRY37irq6vSdV2T6+rqqvXj8wtHPr7x7R8/fpTr6+sm20dHR2U8HjfZXtfP25GPwdt939dSSpOr7/tmz72un7cjH2EFiBVCiBVCiBVCiBVCiBVCiBVCiBVCiBVCiBVCiBVCiBVCiBVCiBVCiBVCiBVCiBVCiBVCiBVCiBVCiBVCiBVCiBVCiBVCiBVCiBVCiBVCiBVCbDznzXd3d+Xr168vdS9LffjwocluKW2fu7VWz721tdVk91+tnvvu7u7R156M9dcjHw8ODpr9EIdhKLPZrMn2aDRq9txHR0el7/tm25ubm022R6NRs897GIbm/1gs8mSstdYvpZQvpZTy8ePH+vDw8OI3tcj+/n45PT1tsn1xcVFaPffNzU2ZTqdNtvu+L4eHh022t7a2mn3es9msfP/+vcn2Mv5mhRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRDPOvJxPB43Oye15ZGP7969K/v7+022R6NR0yMfx+Nxk+2Wn/cwDM1+z5f+ntVa//iaTCa1lcvLy7Xc7vu+llKaXH3fN3vudf28fza2sD9fgyGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCFEV2td/ob/Hvk4OT8/f437+s0wDGVnZ8e27ZXenk6nZT6fdwtffOx4uUWXIx9t235ZjnyEFSBWCCFWCCFWCCFWCCFWCCFWCCFWCCFWCCFWCCFWCCFWCCFWCCFWCCFWCCFWCCFWCCFWCCFWCCFWCCFWCCFWCCFWCCFWCCFWCCFWCCFWCCFWCCFWCOHIR9u239C2Ix9t2w7ZduQjrACxQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQoiNp97w65GPpZSh67q/X/aWHnVQSrm1bXvFt/967IUnz2d9K7qum9daj23bXtdtX4MhhFghRFKsX2zbXuftmL9ZYd0l/c8Ka02sEEKsEEKsEEKsEOIfmbExjTLsIasAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "maze = np.array([\n",
    "    [ 1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "])\n",
    "\n",
    "print('Shape of maze:', maze.shape)\n",
    "flags = [(3,0), (3,2), (3,4), (3,6)]\n",
    "\n",
    "env = Tmaze(maze, flags)\n",
    "show_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training 7x7 maze\n",
    "\n",
    "Here are what we are doing:\n",
    "1. Build our DQN\n",
    "2. Create a Q training object\n",
    "3. Call the train() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\admin\\anaconda3\\envs\\rlcomp2020\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch:   0/199 | Loss: 0.0009 | Episodes:   74 | Wins:  0 | flags: 3 | e: 0.800 | time: 3.4 seconds\n",
      "Epoch:   1/199 | Loss: 0.0009 | Episodes:   20 | Wins:  0 | flags: 3 | e: 0.800 | time: 5.0 seconds\n",
      "Epoch:   2/199 | Loss: 0.0009 | Episodes:  163 | Wins:  0 | flags: 1 | e: 0.800 | time: 20.7 seconds\n",
      "Epoch:   3/199 | Loss: 0.0013 | Episodes:   24 | Wins:  0 | flags: 3 | e: 0.800 | time: 23.0 seconds\n",
      "Epoch:   4/199 | Loss: 0.0008 | Episodes:  193 | Wins:  0 | flags: 1 | e: 0.800 | time: 41.7 seconds\n",
      "Epoch:   5/199 | Loss: 0.0011 | Episodes:  501 | Wins:  1 | flags: 0 | e: 0.735 | time: 90.3 seconds\n",
      "Epoch:   6/199 | Loss: 0.0005 | Episodes:   96 | Wins:  1 | flags: 2 | e: 0.735 | time: 99.6 seconds\n",
      "Epoch:   7/199 | Loss: 0.0002 | Episodes:   52 | Wins:  1 | flags: 3 | e: 0.735 | time: 104.6 seconds\n",
      "Epoch:   8/199 | Loss: 0.0002 | Episodes:   58 | Wins:  1 | flags: 1 | e: 0.735 | time: 110.2 seconds\n",
      "Epoch:   9/199 | Loss: 0.0004 | Episodes:   60 | Wins:  1 | flags: 1 | e: 0.735 | time: 115.9 seconds\n",
      "Epoch:  10/199 | Loss: 0.0009 | Episodes:  257 | Wins:  2 | flags: 0 | e: 0.711 | time: 140.7 seconds\n",
      "Epoch:  11/199 | Loss: 0.0043 | Episodes:   93 | Wins:  3 | flags: 0 | e: 0.694 | time: 149.8 seconds\n",
      "Epoch:  12/199 | Loss: 0.0010 | Episodes:  157 | Wins:  3 | flags: 1 | e: 0.694 | time: 166.0 seconds\n",
      "Epoch:  13/199 | Loss: 0.0011 | Episodes:   48 | Wins:  3 | flags: 1 | e: 0.694 | time: 171.0 seconds\n",
      "Epoch:  14/199 | Loss: 0.0007 | Episodes:   55 | Wins:  3 | flags: 2 | e: 0.694 | time: 176.8 seconds\n",
      "Epoch:  15/199 | Loss: 0.0003 | Episodes:  122 | Wins:  4 | flags: 0 | e: 0.680 | time: 189.1 seconds\n",
      "Epoch:  16/199 | Loss: 0.0004 | Episodes:  266 | Wins:  5 | flags: 0 | e: 0.668 | time: 217.2 seconds\n",
      "Completed training at epoch: 16\n"
     ]
    }
   ],
   "source": [
    "model = build_model(env)\n",
    "\n",
    "qt = Qtraining(\n",
    "    model,\n",
    "    env,\n",
    "    n_epoch = 200,\n",
    "    max_memory = 500,\n",
    "    data_size = 100,\n",
    "    name = 'model_1'\n",
    ")\n",
    "\n",
    "qt.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model\n",
    "\n",
    "It took about 3 minutes to train the model. The first thing we should do is to save our model to disk. We use the standard HDF5 and JSON file formats used by Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qt.save('t1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
